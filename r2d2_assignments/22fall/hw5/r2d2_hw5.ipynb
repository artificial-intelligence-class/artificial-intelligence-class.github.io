{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "r2d2_hw5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9wp38iEgQJa"
      },
      "source": [
        "**In this homework, you will implement several AI models to conduct the intent detection task.**\n",
        "![alt text](https://i.ibb.co/fXmYHRq/ec5.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgSA_YXpHwFI"
      },
      "source": [
        "# Part 0: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuPUTB25ghjz"
      },
      "source": [
        "In this section, you will have a general idea of how the data looks like and do some simple transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKbddkNKfmNE"
      },
      "source": [
        "# download the data\n",
        "!wget \"https://drive.google.com/uc?export=download&id=1dLUN9oSB4u27NOleYE-Uksoh6RNQlZbi\" -O sample.p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4at6WgHmCra"
      },
      "source": [
        "# test sentences for evaluation\n",
        "!wget \"https://drive.google.com/uc?export=download&id=1gEW_qY5x8uPAhriiobubheYo6FC35btQ\" -O test_sentences.p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYqcSyQsmgf1"
      },
      "source": [
        "import pickle\n",
        "samples = pickle.load(open(\"sample.p\", \"rb\"))\n",
        "test_sentences = pickle.load(open(\"test_sentences.p\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AicwRkV-mzqj"
      },
      "source": [
        "###data structure###\n",
        "### [[sentence, label]] ###\n",
        "print(samples[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1m_-5m2eymC"
      },
      "source": [
        "There are nine categories for these sentences, which are 'no', 'driving', 'light', 'head', 'state', 'connection', 'stance', 'animation' and 'grid'. The mapping from index to category name are shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YapxZx0sWhDg"
      },
      "source": [
        "ind2cat = {0: 'no', 1: 'driving', 2: 'light', 3: 'head', 4: 'state', 5: 'connection', 6: 'stance', 7: 'animation', 8: 'grid'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1hHO6pInCH3"
      },
      "source": [
        "### Distribution on categories ###\n",
        "cat2sentence = {}\n",
        "for sample in samples:\n",
        "  sentence = sample[0]\n",
        "  cat = ind2cat[sample[1]]\n",
        "  if cat not in cat2sentence:\n",
        "    cat2sentence[cat] = [sentence]\n",
        "  else:\n",
        "    cat2sentence[cat].append(sentence)\n",
        "\n",
        "print(\"number of sentences for each category\")\n",
        "for cat, sentences in cat2sentence.items():\n",
        "  print(cat, \": \", len(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blzu6p9hoJgi"
      },
      "source": [
        "### Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReEMaskjoMZt"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "SENTENCES = [sample[0] for sample in samples]\n",
        "LABELS = [sample[1] for sample in samples]\n",
        "X_train, X_val, y_train, y_val = train_test_split(SENTENCES, LABELS, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hh5RkEiHrt8"
      },
      "source": [
        "### Clean Text\n",
        "Write a tokenization function clean(sentence) which takes as input a string of text and returns a list of tokens derived from that text. Here, we define a token to be a contiguous sequence of non-whitespace characters. We will remove punctuation marks and convert the text to lowercase. Hint: Use the built-in constant string.punctuation, found in the string module, and/or python's regex library, re."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM0rg6vdHmxy"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = stopwords.words('english')\n",
        "\n",
        "def clean(sentence):\n",
        "  '''1. tokenize the sentence (remove punctuation)\n",
        "     2. remove the stop words\n",
        "     3. convert all words to lowercase'''\n",
        "  pass\n",
        "\n",
        "X_train_token = [clean(sentence) for sentence in X_train]\n",
        "X_val_token = [clean(sentence) for sentence in X_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRb6Z7qBNgsm"
      },
      "source": [
        "max_len = 0# Find the maximum length of tokens in train/val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CskkruUAMMfI"
      },
      "source": [
        "### Build a Vocabulary\n",
        "Build a vocabulary to map each word to an index, you need to first find the unique words in train/val set.\n",
        "\n",
        "Once you build a vocabulary, it's better to save it to a file for future use. Because the vocabulary may change each time you run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAxoqREjMMCI"
      },
      "source": [
        "word_count = {} # count the frequency of each word\n",
        "word2ind = {} # build your vocabulary\n",
        "vocab_size = len(word2ind)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuybRo1HqO-"
      },
      "source": [
        "# Part 1: Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opl1oMHFN0Ph"
      },
      "source": [
        "### Convert token to vector\n",
        "Convert each list of tokens into an array use the vocabulary you built before. The length of the vector is the max_len and remember to do zero-padding if a list's lenghth is smaller than max_len."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5zwLf-SOU0W"
      },
      "source": [
        "def vectorize(tokens, max_len, word2ind):\n",
        "  '''\n",
        "  Input: list of tokens\n",
        "  Output: 1D numpy array (length = max_len)\n",
        "  '''\n",
        "  pass\n",
        "\n",
        "X_train_array = np.array([vectorize(tokens, max_len, word2ind) for tokens in X_train_token])\n",
        "X_val_array = np.array([vectorize(tokens, max_len, word2ind) for tokens in X_val_token])\n",
        "assert X_train_array.shape[-1] == max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATfeiT7IUftX"
      },
      "source": [
        "### One-hot label\n",
        "Convert the scalar label to 1D array (length = 9), e.g 0 -> array([1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gru9TAqnUfYh"
      },
      "source": [
        "y_train_onehot = \n",
        "y_val_onehot = \n",
        "assert y_train_onehot.shape[1] == 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsp0pgj6PI6A"
      },
      "source": [
        "### Build the Recurrent Neural Network\n",
        "Now it's time to build the RNN network to do the classification task, you could just refer to this [official document](https://www.tensorflow.org/guide/keras/rnn).\n",
        "\n",
        "You will need the Embedding layer, RNN layer and Dense layer, your last layer should project to the number of labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ODSzrR2RbbR"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "# Embedding Layer, Input Dimension = vocab_size, Output Dimension = 64\n",
        "\n",
        "# Two LSTM layers with 64 Units\n",
        "\n",
        "# Dense to the number of classes with softmax activation function\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oLUl6dVYTIN"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X_train_array, y_train_onehot, batch_size=16, epochs=10, validation_data=(X_val_array, y_val_onehot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH1Xe7EMlITl"
      },
      "source": [
        "### Evaluate on the test sentences\n",
        "Now run your model to predict on the test sentences, you need to do the preprocessing on these sentences first and save your prediction to a list of labels, e.g [0, 2, 1, 5, ....]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBKwqNUElnyu"
      },
      "source": [
        "test_prediction = []\n",
        "#TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbZmj4Dolo0-"
      },
      "source": [
        "# Save the results and upload to Gradescope\n",
        "pickle.dump(test_prediction, open(\"rnn.p\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8v6GBKws0Z7"
      },
      "source": [
        "#Part 2. Word Embedding via pymagnitude\n",
        "Instead of using the vocabulary to convert word to number, you could use pretrained word embeddings to do the task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbMVzcGDucgZ"
      },
      "source": [
        "! echo \"Installing Magnitude.... (please wait, can take a while)\"\n",
        "! (curl https://raw.githubusercontent.com/plasticityai/magnitude/master/install-colab.sh | /bin/bash 1>/dev/null 2>/dev/null)\n",
        "! echo \"Done installing Magnitude.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VzKjJNNxkYH"
      },
      "source": [
        "Next, you'll need to download a pre-trained set of word embeddings. We'll get a set trained with Google's word2vec algorithm, which we discussed in class. [Here](https://gitlab.com/Plasticity/magnitude), you can check the full list of available embeddings, feel free to try different embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfDkasoHxLjD"
      },
      "source": [
        "# Download Pretrained Word-Embedding\n",
        "! wget http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngFFKNj8yAU5"
      },
      "source": [
        "# Load the embedding\n",
        "from pymagnitude import *\n",
        "vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\") \n",
        "D = vectors.query(\"cat\").shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pdRsfIuVxIg"
      },
      "source": [
        "### Convert tokens to embeddings\n",
        "You could now use the pymagnitude to query each token and convert them to a list of embeddings. Note that you need to do zero padding to match the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0LMW9_OXjs-"
      },
      "source": [
        "def embedding(list_tokens, max_len, vectors, D):\n",
        "  '''\n",
        "  return an array with the shape (n_of_samples, max_len, D)\n",
        "  '''\n",
        "  pass\n",
        "X_train_embedding = embedding(X_train_token, max_len, vectors, D)\n",
        "X_val_embedding = embedding(X_val_token, max_len, vectors, D)\n",
        "\n",
        "assert X_train_embedding.shape[-1] == D\n",
        "assert X_train_embedding.shape[-2] == max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxoxUCB8YPul"
      },
      "source": [
        "### Build the RNN model\n",
        "Similar to Part 1, build a RNN model using your new embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER6xPrArYPLb"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "#TODO\n",
        "# LSTM Layer with input shape (max_len, D), output shape (max_len, 256)\n",
        "\n",
        "# LSTM Layer with 128 units\n",
        "\n",
        "# Dense to 64 with tanh activation function\n",
        "\n",
        "# Dense to number of classes with softmax function\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDyV_5F2kwFT"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X_train_embedding, y_train_onehot, batch_size=16, epochs=10, validation_data=(X_val_embedding, y_val_onehot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGEJ-FtZl3nN"
      },
      "source": [
        "### Evaluate on the test sentences\n",
        "Now run your model to predict on the test sentences, you need to do the preprocessing on these sentences first and save your prediction to a list of labels, e.g [0, 2, 1, 5, ....]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8g9CBNAl8Eo"
      },
      "source": [
        "test_prediction = []\n",
        "#TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAp7kXXpl_L3"
      },
      "source": [
        "# Save the results and upload to Gradescope\n",
        "pickle.dump(test_prediction, open(\"embedding.p\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9dazGvsOzs7"
      },
      "source": [
        "# Part 3: BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQZlpz9xMqFo"
      },
      "source": [
        "In this part, you will use the BERT pipeline to further improve the performance.\n",
        "\n",
        "This part is open-ended, we just provide one example of using BERT, feel free to find other tutorial online to customize on this task.\n",
        "\n",
        "[Here](https://huggingface.co/models) is the list of all existing models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuQDOJNpONp5"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt7_h3tkOili"
      },
      "source": [
        "from transformers import *\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") #feel free to change the model\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59drUhQZO7ep"
      },
      "source": [
        "### Use BERT Tokenizer to preprocess the data\n",
        "The BERT Tokenizer will return a dictionary which contains 'input_ids', 'token_type_ids' and 'attention_mask', we will use the 'input_ids' and 'attention_mask' later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HJrt4odOx-8"
      },
      "source": [
        "# Test the tokenizer\n",
        "sent = X_train[0]\n",
        "tokenized_sequence= bert_tokenizer.encode_plus(sent,add_special_tokens = True,\n",
        "                                              max_length =30,pad_to_max_length = True, \n",
        "                                              return_attention_mask = True)\n",
        "print(tokenized_sequence)\n",
        "print(bert_tokenizer.decode(tokenized_sequence['input_ids']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y1cnxanRD1d"
      },
      "source": [
        "Use the bert tokenizer described above, encode the training and validations sentences, note that the max length should be 64."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpBf_0Z_P4oz"
      },
      "source": [
        "def BERT_Tokenizer(sentences):\n",
        "  '''Input: list of sentences\n",
        "     Output: two numpy array\n",
        "  '''\n",
        "  pass\n",
        "\n",
        "X_train_ids, X_train_masks = BERT_Tokenizer(X_train)\n",
        "X_val_ids, X_val_masks = BERT_Tokenizer(X_val)\n",
        "y_train_array = np.array(y_train)\n",
        "y_val_array = np.array(y_val)\n",
        "assert X_train_ids.shape[-1] == 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MypXAdRKR0Cp"
      },
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6,epsilon=1e-08)\n",
        "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXA2UDc3SaAH"
      },
      "source": [
        "bert_model.fit([X_train_ids,X_train_masks],y_train_array,batch_size=16,epochs=5,validation_data=([X_val_ids,X_val_masks],y_val_array))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpWa8cyTXfV"
      },
      "source": [
        "### Evaluate on test sentences\n",
        "Again, use BERT to predict on the test sentences and submit to Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkNQDrOYTkf1"
      },
      "source": [
        "test_prediction = []\n",
        "#TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3MItfdrTp8C"
      },
      "source": [
        "pickle.dump(test_prediction, open(\"bert.p\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH1nb2_NrEv0"
      },
      "source": [
        "# Part 4: Write your own commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK3_t0qzrNqs"
      },
      "source": [
        "Please write 10 sentences for each category, this will be very helpful for future students!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b71YkD41rMrc"
      },
      "source": [
        "my_commands = {'no': [], \n",
        "               'driving': [], \n",
        "               'light': [],\n",
        "               'head': [],\n",
        "               'state': [],\n",
        "               'connection': [], \n",
        "               'stance': [], \n",
        "               'animation': [],\n",
        "               'grid': []}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAT6yk_Ar-aY"
      },
      "source": [
        "pickle.dump(my_commands, open(\"my_commands.p\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}