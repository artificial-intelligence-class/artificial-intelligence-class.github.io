{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Large Language Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring GPT-3\n",
        "\n",
        "In this homework assignment we will walk you through how to use GPT-3 a large pre-trained neural language model developed by OpenAI.  \n",
        "\n",
        "You will learn about the following topics:\n",
        "* Prompts and completions.  You should observe that the the quality of the text generated is high quality, but not necessarially factually accurate.\n",
        "* Probabilities.  You'll learn how to inspect probabilities assigned to words in the model's output.\n",
        "* Few shot learning.  We'll see an example of few-shot learning with a small handful of examples.\n",
        "* Zero shot learning.  We will explore the zero-shot capabilities of pre-trained LMs.  You'll design zero-shot prompts for\n",
        "1. summarization\n",
        "2. question-answering\n",
        "3. simplification\n",
        "4. translation\n",
        "* How to fine tune a model.  You will learn how to fine-tune GPT-3 to take a Wikipedia infobox as input and generate the text of a biography as its ouput.  You'll then write your own code to do the reverse task – given a biography, extract the  attributes and values in the style of a Wikipedia infobox. \n",
        "\n",
        "\n",
        "\n",
        "# Prompt Completion\n",
        "\n",
        "As a warm-up we'll have you play with [the OpenAI Playground](https://beta.openai.com/playground).  Try inputting this prompt:\n",
        "\n",
        "> One of my favorite professors at the University of Pennsylvania is \n",
        "\n",
        "And the click the \"Submit\" button to generate a completion.\n",
        "\n",
        "Copy and paste the text below (including your prompt). \n",
        "\n",
        "You might notice that the text that GPT-3 generates ends mid-sentence.  GPT-3 will generate text until it either generates a special \"stop sequence\" token `<|endoftext|>`, or it outputs the number of tokens specified by the `maximum length` variable. \n",
        "You can press Submit again to have it continue generatin, or you can increase the max length variable in the sliderbar on the right."
      ],
      "metadata": {
        "id": "pjqy7heO706G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "favorite_professor_completion_1 = \"\"\"\n",
        "One of my favorite professors at the University of Pennsylvania is COPY AND PASTE THE COMPLETION HERE\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZwhSf4Af79Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-3 generates fluent text, but it is not always grounded in fact.  Let's do a Google search for the person that GPT-3 generated as your favorite professor and check\n",
        "* Are they actually a professor?\n",
        "* Where do they work?"
      ],
      "metadata": {
        "id": "nhYnYplw8K4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the professor's name\n",
        "professor_name_1 = \"\"\n",
        "\n",
        "# Do a Google search and answer these questions\n",
        "actually_a_professor_1 = False\n",
        "\n",
        "# Insitituion where they work\n",
        "instituion_1 = \"\""
      ],
      "metadata": {
        "id": "X8-RRIm8-W_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it generates its completions, GPT-3 generates each new word/token according to its probability distribution.  It draws each word at random in proportion to its propability.  That randomness means that it can generate different completions. You can re-generate and get different completions each time.\n",
        "\n",
        "Generate another 4 completions for the professor prompt:\n",
        "\n",
        "> One of my favorite professors at the University of Pennsylvania is \n",
        "\n",
        "and do Google searches for them.\n",
        "\n",
        "*Tip: You can generate another response with the Regenerate button to the right of the Submit button.  The Regenerate button has a recycle symbol on it.*"
      ],
      "metadata": {
        "id": "37sJsYna_jRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "favorite_professor_completion_2 = \"\"\"\n",
        "COPY AND PASTE THE 2ND COMPLETION HERE\n",
        "\"\"\"\n",
        "\n",
        "favorite_professor_completion_3 = \"\"\"\n",
        "COPY AND PASTE THE 3RD COMPLETION HERE\n",
        "\"\"\"\n",
        "\n",
        "favorite_professor_completion_4 = \"\"\"\n",
        "COPY AND PASTE THE 4TH COMPLETION HERE\n",
        "\"\"\"\n",
        "\n",
        "favorite_professor_completion_5 = \"\"\"\n",
        "COPY AND PASTE THE 5TH COMPLETION HERE\n",
        "\"\"\"\n",
        "\n",
        "# Do a Google search for these professors\n",
        "\n",
        "professor_name_2 = \"\"\n",
        "actually_a_professor_2 = False\n",
        "instituion_2 = \"\"\n",
        "\n",
        "professor_name_3 = \"\"\n",
        "actually_a_professor_3 = False\n",
        "instituion_3 = \"\"\n",
        "\n",
        "professor_name_4 = \"\"\n",
        "actually_a_professor_4 = False\n",
        "instituion_4 = \"\"\n",
        "\n",
        "professor_name_5 = \"\"\n",
        "actually_a_professor_5 = False\n",
        "instituion_5 = \"\""
      ],
      "metadata": {
        "id": "zex_KrxrAVGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probabilities\n",
        "\n",
        "Just like with the n-gram language models that we stuided earlier in the course, neural language models like GPT-3 assign probabilities to each token in a sequence.  \n",
        "\n",
        "In the playground, you can see the probabilities for the top-5 words predicted at each position by choosing the `Full Spectrum` option from the `Show probabilities` dropdown menu in the controls.  Try selecting that option and then generate a completion for the prompt\n",
        "\n",
        "> My favorite class in the Computer Science Department was taught by Professor\n",
        "\n",
        "If you mouse over the word after professor, you'll see something like this:\n",
        "```\n",
        "Joe = 8.21%\n",
        "John = 4.25%\n",
        "Nancy = 2.27%\n",
        "David = 2.09%\n",
        "Barbara = 2.05%\n",
        "Total: -2.50 logprob on 1 tokens\n",
        "(18.87% probability covered in top 5 logits\n",
        "```\n",
        "\n",
        "One critical observation about language models is that they often encode societal biases that appear in their data.  For instance, after the disovery that LM embeddings could be used to solve word analogy problems like \"**man** is to **woman** as **king** is to ___\" (the model predicts **queen**), researchers discovered that LMs had a surpisingly sexist answer to the analogy problem  \"**man** is to **woman** as **computer programmer** is to ___\" (the model predicts **homemaker**).  These kinds of biases are prevelant and pernicious. \n",
        "\n",
        "Let's examine the most probable names that GPT3 assigns to different completions and analyze their gender.  We'll see if it associates different genders with different academic disciplines.  (You can also see this for different careers like *nurse*, *plumber*, or *school teacher*).\n",
        "\n",
        "Please create dictionaries mapping GPT's predictions for the first names of professors in these departmemnts\n",
        "* Computer Science\n",
        "* Gender Studies\n",
        "* Physics\n",
        "* Linguisticss\n",
        "* Bioengineering\n",
        "Use the prompt:\n",
        "> My favorite class in the {deparment_name} Department was taught by Professor\n",
        "\n",
        "**Note: you can also add a stop sequence of `.` to get the model to complete only a single sentence.**\n",
        "\n"
      ],
      "metadata": {
        "id": "y0bIhdi24moc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Classify each name as male, female, partial word, or unknown\n",
        "computer_science_genders = {\n",
        "  \"Joe\" : \"male\",\n",
        "  \"John\" : \"male\",\n",
        "  \"Nancy\" : \"female\",\n",
        "  \"David\" : \"male\",\n",
        "  \"Barbara\" : \"female\",\n",
        "}\n",
        "\n",
        "gender_studies_genders = {\n",
        "  \"TODO\" : \"TODO\",\n",
        "}\n",
        "\n",
        "physics_genders = {\n",
        "  \"TODO\" : \"TODO\",\n",
        "}\n",
        "\n",
        "lingusitics_genders = {\n",
        "  \"TODO\" : \"TODO\",\n",
        "}\n",
        "\n",
        "bioengineering_genders = {\n",
        "  \"TODO\" : \"TODO\",\n",
        "}"
      ],
      "metadata": {
        "id": "rj2Dt1BL4l-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(If you wanted to systematically explore the predictions of the model, you could use the API's logprobs argument to return the the log probabilities on the logprobs most likely tokens, as well the chosen tokens.)"
      ],
      "metadata": {
        "id": "DckHlrauFYdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Learning\n",
        "\n",
        "One of the remarkable properties of large language models is a consequence of the fact that they have been trained on so much language data.  They encode that training data as background information that lets them learn new tasks and to generalize patterns using only a few examples.  This is called \"Few shot learning\".\n",
        "\n",
        "Here is an example.  Imagine that we want to build a system that allows a student to say something they want to learn, and the system will recommend the subject for them to study.  Here are examples of inputs and outputs to our program:\n",
        "\n",
        "```\n",
        "how to program in Python - computer science\n",
        "factors leading up to WW2 - history\n",
        "branches of government - political science\n",
        "Shakespeare's plays - English\n",
        "cellular respiration - biology\n",
        "respiratory disease - medical\n",
        "how to sculpt - art\n",
        "```\n",
        "\n",
        "We can use these 7 examples (and probably fewer!) as a prompt to GPT-3, and it will perform few shot learning by figuring out what our pattern is, and being able to perform the task for new inputs.\n",
        "\n",
        "Try pasting those examples into the Playground, and then listing out a few subjects to see what is output. \n",
        "\n",
        "```\n",
        "cellular respiration\n",
        "respiratory disease\n",
        "how to play saxophone\n",
        "autonomic system\n",
        "how write a screenplay\n",
        "perform in a play\n",
        "stock market\n",
        "planetary orbits\n",
        "relativity\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qV65mL6va3lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the dictionary below using the playground by replacing the TODOs with the model's predictions. "
      ],
      "metadata": {
        "id": "CRMLTBoxe6V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_subject_classification_results = {\n",
        "  \"cellular respiration\" : \"TODO\",\n",
        "  \"respiratory disease\" : \"TODO\",\n",
        "  \"how to play saxophone\" : \"TODO\",\n",
        "  \"autonomic system\" : \"TODO\",\n",
        "  \"how write a screenplay\" : \"TODO\",\n",
        "  \"perform in a play\" : \"TODO\",\n",
        "  \"stock market\" : \"TODO\",\n",
        "  \"planetary orbits\" : \"TODO\",\n",
        "  \"relativity\" : \"TODO\",\n",
        "}"
      ],
      "metadata": {
        "id": "cTaSq85Ka8WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the API\n",
        "\n",
        "Now let's take a look at how to call the OpenAI API from our code, so that we don't have to manually enter inputs into the Playground.  \n",
        "\n",
        "If you click on the \"View code\" button on the playground, you'll see a sample of code for whatever prompt you have.  For example, here's the code that we have for our few-shot learning that generates a subject to study for a topic that someone is interested in:\n",
        "\n",
        "```python\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-002\",\n",
        "  prompt=\"how to program in Python - computer science\\nfactors leading up to WW2 - history\\nbranches of government - political science\\nShakespeare's plays - English\\ncellular respiration - biology\\nrespiratory disease - medical\\nhow to sculpt - art\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=256,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "```\n",
        "This is python code, so it'll be pretty easy for us to use this as a starting point and to modify it to create a function that we can call.\n",
        "\n",
        "\n",
        "First, you'll need install the OpenAPI via pip.  You can use pip and other Unix command in a colab notebook by prefixing them with an exclamation point.  (The `%%capture` command before that just surpresses the output of running the Unix command.  You can remove it if you want to see the progress of the command).\n"
      ],
      "metadata": {
        "id": "4VpROz_FfJZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "396iGnE4ra9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you will enter your secret key for the OpenAI API, then you can find your OpenAI API key [here](https://beta.openai.com/account/api-keys).  \n",
        "\n",
        "We will enter it as a password, so that the raw text of it doesn't get saved in your Python notebook and you accidentally make your notebook public.  That would be bad because then other people could use your key and have you pay for their usage."
      ],
      "metadata": {
        "id": "9jdqGfOyrmhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "print('Enter OpenAI API key:')\n",
        "openai_api_key = getpass()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=openai_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PIBX87qrlDd",
        "outputId": "4d0b575f-2e1a-48f4-8734-b05e5721b66f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key:\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's write a function that takes a topic as input and then outputs a subject to study if you want to learn about that topic."
      ],
      "metadata": {
        "id": "yt7VVKVtvcCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import time\n",
        "\n",
        "def generate_subject_few_shot(topic):\n",
        "  few_shot_prompt = \"\"\"how to program in Python - computer science\n",
        "factors leading up to WW2 - history\n",
        "branches of government - political science\n",
        "Shakespeare's plays - English\n",
        "cellular respiration - biology\n",
        "respiratory disease - medical\n",
        "how to sculpt - art\n",
        "\"\"\"\n",
        "\n",
        "  response = openai.Completion.create(\n",
        "      model=\"text-davinci-002\",\n",
        "      prompt=few_shot_prompt + topic + \" - \", # We'll append our topic and a dash to the end of the few shot prompt.\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stop=[\"\\n\"]\n",
        "  )\n",
        "  # I recommend putting a short wait after each call, \n",
        "  # since the rate limit for the platform is 60 requests/min.\n",
        "  # (This increases to 3000 requests/min after you've been using the platform for 2 days).\n",
        "  time.sleep(1)\n",
        "\n",
        "  # the response from OpenAI's API is a JSON object that contains \n",
        "  # the completion to your prompt plus some other information.  Here's how to access\n",
        "  # just the text of the completion. \n",
        "  return response['choices'][0]['text'].strip()\n",
        "\n",
        "topic = \"cellular respiration\"\n",
        "generate_subject_few_shot(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "o9-oV9mnvmtD",
        "outputId": "d2aa722a-9fc1-416e-9346-ce76c68cdc5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'biology'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it!  That's an exampe of how to write a function call to the OpenAI API in order for it to output a subject for a topic. \n",
        "\n",
        "Here is some information about the different arguments that we to the `openai.Completion.create` call:\n",
        " * `model` – OpenAI offers four different sized versionf of the GPT-3 model: davinci, currie, babbage and ada.  Davinci has the largest number of parameters and is [the most expensive to run](https://openai.com/api/pricing/).  Ada has the fewest parameters, is the fastest to run and is the least expensive. \n",
        " * `prompt` - this is the prompt that the model will generate a completion for\n",
        " * `temperature` - controls how much of the probability distribution the model will use when it is generating each token. 1.0 means that it samples from the complete probability distrubiton, 0.7 means that it drops the bottom 30% of the least likely tokens when it is sampling. 0.0 means that it will perform deterministically and always output the single most probable token for each context. \n",
        " * `top_p` - is an alternative way of controling the sampling. \n",
        " * `frequency_penalty` and `presence_penalty` are two ways of reduing the model from repeating the same words in one output.  You can set these to be >0 if you're seeing a lot of repetition in your output. \n",
        " * `max_tokens` is the maximum length in tokens that will be output by calling the function.  A token is a subword unit.  There are roughly 2 or 3 tokens per word on average.\n",
        " * `stop` is a list of stop sequences.  The model will stop generating output once it generates one of these strings, even if it hasn't reached the max token length. By default this is set to a special token `<|endoftext|>`.\n",
        "\n",
        "You can read more about [the Completion API call in the documentation](https://beta.openai.com/docs/api-reference/completions)."
      ],
      "metadata": {
        "id": "Wmr06VEzxlnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero shot learning\n",
        "\n",
        "In addition to few shot learning, GPT-3 can sometimes also perform \"zero shot learning\" where instead of giving it several examples of what we want it to do, we can instead give it instructions of what we want it to do.\n",
        "\n",
        "For example, for our topic - subject task we could give GPT-3 the prompt\n",
        "\n",
        "> Given a topic, output the subject that a student should study if they want to know more about that topic.\n",
        "\n",
        "Then if we append \n",
        "> cellular respiration -\n",
        "\n",
        "GPT3 will output biology.\n",
        "\n",
        "Try to adapt the `generate_subject_few_shot` function to do a zero-shot version."
      ],
      "metadata": {
        "id": "h5iKKme91RMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_subject_zero_shot(topic):\n",
        "  # TODO - write this function\n",
        "  pass"
      ],
      "metadata": {
        "id": "aLve17Cl3d2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very cool recent finding is that training proceedure for large language models can be changed to improve this instruction following behavior.  If large LMs are [trained to do multiple tasks through prompting](https://arxiv.org/abs/2110.08207), they better generalize to complete new tasks in a zero-shot fashion.  The current version of GPT3 (text-davinci-2) uses this kind of training.\n",
        "\n",
        "Try writing zero-shot prompts to do the following tasks:\n",
        "1. Summarize a Wikipedia article.\n",
        "2. Answer questions about an article.\n",
        "3. Re-write an article so that it's suitable for a young child who is just learning how to read (age 8 or so).\n",
        "4. Translate an article from Russian into English.\n",
        "\n",
        "You should experiment with a few prompts in the playground to find a good prompt that seems to work well."
      ],
      "metadata": {
        "id": "O3eB1xva5Nr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(article_text):\n",
        "  # TODO - write this function \n",
        "  summary = \"\"\n",
        "  return summary\n",
        "\n",
        "def answer_question(article_text, question):\n",
        "  # TODO - write this function \n",
        "  answer = \"\"\n",
        "  return answer\n",
        "\n",
        "def simplify(article_text):\n",
        "  # TODO - write a function to re-write an article so that it's suitable for a young child.\n",
        "  simplified_article = \"\"\n",
        "  return simplified_article\n",
        "\n",
        "def translate(article_text, source_language, target_language):\n",
        "    # TODO - write a function to translate an article from a source language to a target language.\n",
        "  simplified_article = \"\"\n",
        "  return simplified_article"
      ],
      "metadata": {
        "id": "g5AWEh526gIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show your outputs in your prompts.  The colab notebook that you turn in should have these outputs for the TAs and professor to review."
      ],
      "metadata": {
        "id": "T9oCIWFW7-Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = \"\"\"\n",
        "TODO - copy and paste part of a Wikipedia article here.\n",
        "\"\"\"\n",
        "\n",
        "summarize(article_text)"
      ],
      "metadata": {
        "id": "y8ODUSvg8dbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = \"\"\"\n",
        "TODO - copy and paste part of a Wikipedia article here.\n",
        "\"\"\"\n",
        "questions = [\n",
        "    \"TODO - add questinon 1\",\n",
        "    \"TODO - add questinon 2\",\n",
        "    \"TODO - add questinon 3\",\n",
        "    \"TODO - add questinon 4\",\n",
        "    \"TODO - add questinon 5\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "  answer = answer_question(article_text, question)\n",
        "  print(question)\n",
        "  print(answer)\n",
        "  print('---')\n"
      ],
      "metadata": {
        "id": "N6Y3--nI8h-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_text = \"\"\"\n",
        "TODO - copy and paste part of a Wikipedia article here.\n",
        "\"\"\"\n",
        "\n",
        "simplify(article_text)"
      ],
      "metadata": {
        "id": "5d2SS-fT8klT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "russian_article = \"\"\"\n",
        "TODO - copy and paste part of a Russian langauge Wikipedia article here.\n",
        "\"\"\"\n",
        "\n",
        "source_language = \"Russian\"\n",
        "target_language = \"English\"\n",
        "translate(russian_article, source_language, target_language)"
      ],
      "metadata": {
        "id": "w8f1cGZN8nX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO - Pick your own task\n",
        "\n",
        "For this section you should pick some task that you'd like to have GPT3 do.  Add a description and code to your notebook here.  You should:\n",
        "1. Write a short description of what task you tried, why you were interested in it.\n",
        "2. Give some code so that we can reproduce what you did via an Open API call.  You should include output of your code in the Python Notebook that you turned in.\n",
        "3. Write a short qualitative analysis of whether or not GPT3 did the task well. "
      ],
      "metadata": {
        "id": "mVe3k3sl9p6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO - your task description"
      ],
      "metadata": {
        "id": "Y7uWoojg-YY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO your code"
      ],
      "metadata": {
        "id": "FgwbDBsS-a2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO - write a short paragraph giving your qualitative analysis of how well GPT3 did for your task."
      ],
      "metadata": {
        "id": "v9CjM8hM-cc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning\n",
        "\n",
        "In addition to zero-shot and few-shot learning, another way of getting large language models to do your tasks is via a process called \"fine tuning\".  In fine-tuning the model updates its parameters so that it performs well on many training examples.  The training examples are in the form of input prompts paired with gold standard completions.\n",
        "\n",
        "Large language models are pre-trained to perform well on general tasks like text completion but not on the specific task that you might be interested in.  The models can be fine tuned to perform you task, starting with the model parameters that are good for the general setting, and then updating them to be good for your task. \n",
        "\n",
        "We'll walk through how to fine-tune GPT3 for a task.\n"
      ],
      "metadata": {
        "id": "7x8z9JJnAQs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example, we will show you how to fine tune GPT3 to write biographies. From data in the info boxes in Wikipedia pages.  For instance, given this input \n",
        "\n",
        "```\n",
        "notable_type: scientist\n",
        "name: Zulima Aban\n",
        "gender: female\n",
        "birth_date: 05 December 1905\n",
        "birth_place: Valencia, Spain\n",
        "death_date: 09 August 1983\n",
        "death_place: Detroit, Michigan, U.S.\n",
        "death_cause: Pulmonary embolism\n",
        "occupation: Astronomer\n",
        "fields: Astrophysics, Computer Science, Computer Graphics, Interface Design, Image Synthesis\n",
        "known_for: The Search for Planet Nine\n",
        "hometown: Detroit, Michigan, U.S.\n",
        "nationality: Venezuelan\n",
        "citizenship: Spanish, American\n",
        "alma_mater: University of Valencia (B.Sc.), University of Madrid (Ph.D.)\n",
        "thesis_title: The Formation of Planets by the Accretion of Small Particles\n",
        "thesis_year: 1956\n",
        "doctoral_advisor: Angela Carter\n",
        "awards: Spanish Academy of Science, Spanish Academy of Engineering, German Aerospace Prize, IEEE Medal of Honor, IEEE John von Neumann Medal, IEEE Jack S. Kilby Signal Processing Medal, United Nations Space Pioneer Award, Wolf Prize in Physics\n",
        "institutions: Oberlin College, University of Valencia, Instituto de Astrofísica de Andalucía (CSIC), University of Southern California, Space Telescope Science Institute (STScI)\n",
        "notable_students: Ryan Walls\n",
        "influences: Immanuel Kant, Albert Einstein, Kurt Gödel, Gottfried Leibniz, Richard Feynman, Werner Heisenberg, William Kingdon Clifford, Sir Arthur Eddington\n",
        "influenced: Joseph Weinberg\n",
        "mother: Ana Aban\n",
        "father: Joaquín Aban\n",
        "partner: Georgina Abbott\n",
        "children: Robert, Peter, Sarah\n",
        "```\n",
        "\n",
        "The fine-tuned model will generate this output:\n",
        "\n",
        "> Zulima Aban was a Venezuelan astronomer, who was born on 05 December 1905 in Valencia, Spain to Ana Aban and Joaquín Aban. Her career involved the fields of Astrophysics, Computer Science, Computer Graphics, Interface Design, Image Synthesis. Aban was known for The Search for Planet Nine. Aban went to University of Valencia (B.Sc.), University of Madrid (Ph.D.). Aban's thesis title was The Formation of Planets by the Accretion of Small Particles in 1956. Her doctoral advisor was Angela Carter. Aban received Spanish Academy of Science, Spanish Academy of Engineering, German Aerospace Prize, IEEE Medal of Honor, IEEE John von Neumann Medal, IEEE Jack S. Kilby Signal Processing Medal, United Nations Space Pioneer Award, Wolf Prize in Physics. Aban went to Oberlin College, University of Valencia, Instituto de Astrofísica de Andalucía (CSIC), University of Southern California, Space Telescope Science Institute (STScI). Her notable students were Ryan Walls. Aban was influenced by Immanuel Kant, Albert Einstein, Kurt Gödel, Gottfried Leibniz, Richard Feynman, Werner Heisenberg, William Kingdon Clifford, Sir Arthur Eddington and she infuenced Joseph Weinberg. Aban was married to Georgina Abbott and together had three children, Robert, Peter, Sarah. Aban died on 09 August 1983 in Detroit, Michigan, U.S due to Pulmonary embolism.\n",
        "\n",
        "The dataset that we will use was created for the paper [SynthBio: A Case Study in Human-AI Collaborative Curation of Text Datasets](https://www.cis.upenn.edu/~ccb/publications/synthbio.pdf) by Ann Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris Callison-Burch, Andy Coenen, and Sebastian Gehrmann. It was published in NeurIPS 2021.  The goal of the paper was to create a curated dataset for training large language models on synthetic data with the goal of avoiding the gender and geographic bias that is naturally present in Wikipedia due to cultural and historic reasons. \n"
      ],
      "metadata": {
        "id": "GDez1YvZHFaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ],
      "metadata": {
        "id": "j1WbRlYDIadg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_train.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mrmOrl6Ad0E",
        "outputId": "6b464dc5-0888-40d6-87ae-1eb58dee4af2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-07 23:32:22--  https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5807118 (5.5M) [text/plain]\n",
            "Saving to: ‘SynthBio_train.json’\n",
            "\n",
            "\rSynthBio_train.json   0%[                    ]       0  --.-KB/s               \rSynthBio_train.json 100%[===================>]   5.54M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-08-07 23:32:22 (111 MB/s) - ‘SynthBio_train.json’ saved [5807118/5807118]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a file called 'SynthBio.json' which is a list of json objects.\n",
        "# Pretty the first 5 json examples, nicely formatted.\n",
        "\n",
        "import json\n",
        "import random\n",
        "\n",
        "def load_wiki_bio_data(filename='SynthBio_train.json', num_bios=100, randomized=True):\n",
        "  with open(filename) as f:\n",
        "    synth_bio_data = json.load(f)\n",
        "  random.shuffle(synth_bio_data)\n",
        "  bios = []\n",
        "  for data in synth_bio_data:\n",
        "    notable_type = data['notable_type']\n",
        "    attributes = \"notable_type: {notable_type} | {other_attributes}\".format(\n",
        "        notable_type = notable_type, \n",
        "        other_attributes = data['serialized_attrs']\n",
        "    )\n",
        "    biography = data['biographies'][0]\n",
        "    bios.append((attributes.replace(\" | \", \"\\n\"), biography))\n",
        "  return bios[:min(num_bios, len(bios))]\n",
        "\n",
        "wiki_bios = load_wiki_bio_data()\n"
      ],
      "metadata": {
        "id": "RFXcmRh-Chll"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes, bio = wiki_bios[0]\n",
        "print(attributes)\n",
        "print('---')\n",
        "bio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "hOeMh6leD0gi",
        "outputId": "a8185339-10af-4415-d9d8-93ca6e830e68"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "notable_type: mountaineer\n",
            "name: Olev Vilhelmson\n",
            "gender: male\n",
            "nationality: Latvian\n",
            "birth_date: 07 November 1876\n",
            "birth_place: Jelgava, Latvia\n",
            "death_date: 21 February 1952\n",
            "death_place: Geneva, Switzerland\n",
            "death_cause: pneumonia\n",
            "resting_place: La Chaux-de-Fonds Cemetery\n",
            "start_age: 12\n",
            "notable_ascents: First ascent of Mt. Kebnekaise in 1894, Youngest person to climb All peaks higher than 4,000 meter 1895\n",
            "final_ascent: Highest Peak in Sweden and Scandinavia\n",
            "partnerships: Gustaf Bergmann, Björn Dunker\n",
            "mother: Emilia Vilhelmina Bartlett\n",
            "father: Gustav Vilhelmson\n",
            "partner: Anna Hulthorst\n",
            "children: Vilhelmina Bergman-Malmstroem, Elsa Bergman-Malmstroem\n",
            "---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Olev Vilhelmson (7 November, 1876 - 21 February, 1952) was a Latvia. He was born in Jelgava, Latvia, His start age is 12. He had partnership with Gustaf Bergmann, Bjorn dunker. His father was Gustav Vilhelmson and his mother was Emilia Vilhelmina Bartlett. Olev first ascent was Mt. Kebnekaise in 1894. He was also the youngest person to climb all peaks higher than 4,000 meters in 1895. He died of pneumonia on February 21, 1952 in Geneva, Switzerland. He was buried in the cemetery of Chaux-de-Fonds. He was married to Anna Hulthorst and had two children Vilhelmina Bergman-Malmstroem, Elsa Bergman-Malmstroem.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format Data for Fine-Tuning \n",
        "\n",
        "Below, I show how to format data to fine-tune OpenAI.  The OpenAI API documentation has a [guide to fine-tuning models](https://beta.openai.com/docs/guides/fine-tuning) that you should read.   The basic format of fine-tuning data is a JSONL file (one JSON object per line) with two key-value pairs: `prompt:` and `completion:`.\n",
        "\n",
        "```\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "...\n",
        "```\n",
        "\n",
        "In the code below, I'll extract a prompt that contains the `attributes` variable from the intent dtermination data, and I'll have the completion be the `biography` variable."
      ],
      "metadata": {
        "id": "qDZYB6CW2m7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def create_wikibio_finetuning_data(wikibios, fine_tuning_filename):\n",
        "  fine_tuning_data = []\n",
        "\n",
        "  for attributes, bio in wiki_bios:\n",
        "    prompt = \"{attributes}\\n---\\n\".format(attributes=attributes)\n",
        "    completion = \"Biography: {bio}\\n###\".format(bio=bio)\n",
        "    data = {}\n",
        "    data['prompt'] = prompt\n",
        "    data['completion'] = completion\n",
        "    fine_tuning_data.append(data)\n",
        "\n",
        "  random.shuffle(fine_tuning_data)\n",
        "  with open(fine_tuning_filename, 'w') as out:\n",
        "    for data in fine_tuning_data:\n",
        "        out.write(json.dumps(data))\n",
        "        out.write('\\n')\n",
        "\n",
        "\n",
        "fine_tuning_filename='wikibio_finetuning_data.jsonl'\n",
        "create_wikibio_finetuning_data(wiki_bios, fine_tuning_filename)"
      ],
      "metadata": {
        "id": "2UKlNc01b4LR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll perform fine-tuning with this data using OpenAI. "
      ],
      "metadata": {
        "id": "wEqja42Yc5O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade openai\n",
        "!pip install jsonlines\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "og19yX-Mc4-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've got access to the OpenAI API, you can find your OpenAI API key [here](https://beta.openai.com/account/api-keys)."
      ],
      "metadata": {
        "id": "fE8RjE6SdGGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from getpass import getpass\n",
        "print('Enter OpenAI API key:')\n",
        "openai_api_key = getpass()\n",
        "\n",
        "os.environ['OPENAI_API_KEY']=openai.api_key"
      ],
      "metadata": {
        "id": "g2uAKwEzdGrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head '{fine_tuning_filename}'"
      ],
      "metadata": {
        "id": "P9SVG2fudLK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8451a4a-4246-4b76-8402-32f4deb23e36"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"prompt\": \"notable_type: writer\\nname: Helmut Berger\\ngender: male\\nnationality: German\\nbirth_date: 22 October 1881\\nbirth_place: Schonwald Germany\\ndeath_date: 2 July 1954\\ndeath_place: Locarno Switzerland\\ndeath_cause: hemorrhage, hypertension\\nresting_place: La Chaux-de-Fonds Cemetery\\nalma_mater: University of Freiburg\\neducation: Master of Arts in Philosophy\\noccupation: German philosopher, poet, and translator\\nnotable_works: \\\"Das Wunder\\\", \\\"Der gute Mensch\\\"\\nlanguage: German\\ngenre: poetry\\nawards: Nobel Prize in Literature\\nmother: Anna Augusta Elisabeth B\\u00fclow\\npartner: Bertha Margarete Deysing\\nchildren: Klaus\\n---\\n\", \"completion\": \"Biography: Helmut Berger was a German philosopher, poet, and translator. He was born on October 22, 1881 in Schonwald, Germany. He attended the University of Freiburg, where he earned a Master of Arts in Philosophy. Berger's notable works include \\\"Das Wunder\\\", \\\"Der gute Mensch\\\", \\\"Das Wunder\\\". Berger won the Nobel Prize in Literature. He was married to Bertha Margarete Deysing. Berger died on July 2, 1954 in Locarno, Switzerland.\\n###\"}\n",
            "{\"prompt\": \"notable_type: mountaineer\\nname: Yumi Watanabe\\ngender: female\\nnationality: Japanese\\nbirth_date: 22 October 1917\\nbirth_place: Kumamoto, Japan\\ndeath_date: June 19, 1992\\ndeath_place: K-2, Pakistan\\ndeath_cause: avalanche, fell 1,900m\\nresting_place: Sagarm\\u0101th\\u0101 National Park near Kathmandu\\nstart_age: 28\\nnotable_ascents: Japanese Alps\\nfinal_ascent: K-2 in Pakistan\\nmother: Hiraiwa Watanabe\\nfather: Wataru Watanabe\\npartner: Tadao Ohmori\\nchildren: none\\n---\\n\", \"completion\": \"Biography: Yumi Watanabe was born in Kumamoto, Japan on October 22, 1917. Her mother was Hiraiwa Watanabe, father was Wataru Watanabe and married to Tadao Ohmori. As a child, Watanabe was raised in Kumamoto, Japan. She began climbing at the age of 28. She climbed on Japanese Alps and her final ascent was K-2 in Pakistan. She died on June 19, 1992 in K-2, Pakistan due to avalanche, fell 1,900m and buried at Sagarm\\u0101th\\u0101 National Park near Kathmandu.\\n###\"}\n",
            "{\"prompt\": \"notable_type: spy\\nname: Inoka Madakiry\\ngender: non-binary\\nnationality: Sri Lankan\\nbirth_date: 20 September 1974\\nbirth_place: Colombo, Sri Lanka,\\nserviceyears: 2001-2015\\nknown_for: providing classified information about British weapons to Syria\\ncriminal_penalty: 30 years in prison\\nalma_mater: University of Peradeniya\\noccupation: Defense Advisor\\ncodename: Ikebana\\nallegiance: United States\\nagency: Defense Security Service (DSS)\\noperation: Operation Moby Dick\\nmother: Nanako Madakiry\\nfather: Yosihiko Urabe\\npartner: Kotoburo Abe\\nchildren: Yuki Abe\\n---\\n\", \"completion\": \"Biography: Inoka Madakiry was born on September 20, 1974 in Colombo, Sri Lanka to Nanako and Yosihiko Urabe. Madakiry was a Defense Advisor for the United States, as well as a Defense Security Service officer. In 2001, Madakiry began providing classified information about British weapons to Syria. Their service is from 2001-2015.Madakiry was sentenced to 30 years in prison and their code name is Ikebana. Their alma mater is in University of Peradeniya. Inoka Madakiry married Kotoburo Abe and their is a kid Yuki Abe.\\n###\"}\n",
            "{\"prompt\": \"notable_type: spy\\nname: Nawng Hla Win\\ngender: female\\nnationality: Burmese\\nbirth_date: 25 September 1894\\nbirth_place: Rangoon, British Burma\\ndeath_date: 5 May, 1971\\ndeath_place: Yangon, Burma\\ndeath_cause: kidney failure\\nresting_place: Burma\\nserviceyears: 1911-1945\\nknown_for: served as an ambulance driver in World War One, and spy in World War Two. Known as the \\\"Mother of Burmese\\\" intelligence.\\nalma_mater: Oxford University\\noccupation: head of Burmese intelligence services, and worked for the British.\\ncodename: Win\\nallegiance: British Empire and later, United Kingdom\\nagency: Special Operations Executive\\noperation: Burma campaign\\nmother: Hla Myaing\\nfather: Myaing\\n---\\n\", \"completion\": \"Biography: Born in Rangoon, Burma, on September 25, 1894, Nawng Hla Win was known for her work as a spy and as an ambulance driver during World War One. She later worked for the British as an intelligence agent in Burma. She was known as the \\\"Head of Burmese\\\" intelligence. She died on May 5, 1971 in Yangon, Burma. She is the daughter of Hla Myaing and Myaing.\\n###\"}\n",
            "{\"prompt\": \"notable_type: theologian\\nname: Nora Bouzid\\ngender: female\\nnationality: Algerian\\nbirth_date: 08 January 1937\\nbirth_place: B\\u00e9ja\\u00efa, French Algeria\\ndeath_date: 12 June 2010\\ndeath_place: Montreal, Canada\\ndeath_cause: cancer\\nresting_place: Algiers, Algeria\\nalma_mater: University of Montr\\u00e9al, Universit\\u00e9 de Montr\\u00e9al, Acad\\u00e9mie des Sciences, \\u00c9cole pratique des Hautes \\u00c9tudes\\noccupation: scholar, professor, writer\\nnotable_works: \\u00c9volution et r\\u00e9volution, La religion alg\\u00e9rienne dans la pens\\u00e9e francaise moderne, L'Histoire, culture et civilisation du Maghreb\\nmain_interests: contemporary Algerian culture and history, Maghreb history and religion\\nchildren: none\\n---\\n\", \"completion\": \"Biography: Nora Bouzid born 8 January 1937 was an Algerian scholar, writer. Bouzid was born in B\\u00e9ja\\u00efa. Nora Bouzid notable works include \\u00c9volution et r\\u00e9volution, La religion alg\\u00e9rienne dans la pens\\u00e9e francaise moderne, L'Histoire, culture et civilisation du Maghreb. Nora's main interest are in contemporary Algerian culture and history, Maghreb history and religion. She died on 12 June 2010 in Montreal, Canada.\\n###\"}\n",
            "{\"prompt\": \"notable_type: mountaineer\\nname: Ella Kalnins\\ngender: female\\nnationality: Latvian\\nbirth_date: 04 April 1874\\ndeath_date: 07 January 1922\\ndeath_cause: pneumonia\\nresting_place: Riga, Latvia\\nnotable_ascents: Mt. Denali, Mt. Foraker, Mt. Elbrus, Mt. Baker\\nfinal_ascent: Piramide Vincent\\npartnerships: Willi Unsoeld\\nmother: Sofija Jana Kuldinina\\nfather: Janis Kalnins\\npartner: Robert Peary\\nchildren: Henrix Kalnins, Juli Kalnins, Dzirk Kalnins\\n---\\n\", \"completion\": \"Biography: Ella Kalnins (born 04 April 1874) was a Latvian who made many ascents including Mt. Denali, Mt. Foraker, Mt. Elbrus, Mt. Baker. Her final ascent was Piramide Vincent. She had partnerships with Willi Unsoeld. She was the daughter of Janis Kalnins and Sofija Jana Kuldinina. She was married to Robert Peary and had three children Henrix Kalnins, Juli Kalnins, Dzirk Kalnins. She died of pneumonia on 7 January 1922.\\n###\"}\n",
            "{\"prompt\": \"notable_type: spy\\nname: Kyaing Sein\\ngender: non-binary\\nnationality: Burmese\\nbirth_date: 12 March 1946\\nbirth_place: Rangoon, British Colony of Burma\\ndeath_date: 22 March 1989\\ndeath_place: Yangon, Burma\\ndeath_cause: cancer\\nresting_place: Rangoon General Cemetery in Rangoon, Burma\\nserviceyears: 15 July 1971 - 28 March 1985\\nknown_for: the assassination of Karen rebel leader Saw Ba U Gyi\\ncriminal_penalty: life imprisonment with hard labor\\nalma_mater: Rangoon University majoring in political economics and public administration\\noccupation: officer of the State Law and Order Restoration Council of Burma\\ncodename: White Dragon\\nallegiance: Burmese State Law and Order Restoration Council, under his direct leader General Ne Win\\nagency: State Law and Order Restoration Council Special Operations\\noperation: a covert operation in Thailand\\nmother: Maung Sein\\nfather: U Shwe Myint\\npartner: none\\nchildren: 2 sons\\n---\\n\", \"completion\": \"Biography: Kyaing Sein was born on March 12, 1946 in Rangoon, British Colony of Burma. He was the son of U Shwe Myint and Maung Sein. He attended Rangoon University majoring in political economics and public administration. He was a member of the State Law and Order Restoration Council of Burma. He was a field officer for the State Law and Order Restoration Council Special Operations. Kyaing Sein was a military intelligence operative who served as a spy for the Burmese government during a covert operation in Thailand. He was a key figure in the assassination of Karen rebel leader Saw Ba U Gyi. Kyaing Sein was a military general for the State Law and Order Restoration Council and codename was White Dragon. He was later arrested and sentenced to life imprisonment with hard labor by a special tribunal and imprisoned in the Insein Prison. He was active between the service years of 15 July 1971 - 28 March 1985 and his allegiance was to Burmese State Law and Order Restoration Council, under his direct leader General Ne Win. He died on March 22, 1989 in Yangon, Burma. He was buried in the Rangoon General Cemetery in Rangoon, Burma. He was survived by his two sons.\\n###\"}\n",
            "{\"prompt\": \"notable_type: musician\\nname: Sofia Ortiz\\nbirth_name: Ramona\\nalias: \\u201cLa Reina de los Vallenatos\\u201d/\\u201dThe Queen of Vallenatos\\u201d\\ngender: female\\nbirth_date: 10 November 1887\\nbirth_place: Valledupar, Cesar\\ndeath_date: May 5, 1947\\ndeath_place: Medellin, Colombia\\ndeath_cause: tuberculosis\\nresting_place: Cemetery Jardines de Paz\\ninstrument: accordion\\ngenre: vallenato\\nhometown: Valledupar\\nnationality: Dominican\\ncitizenship: Colombian\\neducation: University of Antioquia, Colombia\\nyears_active: 1915-1947\\nlabel: RCA/Victor\\nassociated_acts: Los Trovadores Zulianos\\nawards: Colombian Songwriters Hall of Fame (2001), Songwriters Hall of Fame (2005), National Order of Merit (Commander) (1987), Grammy Award for Best Tropical Music Album...\\nmother: Mar\\u00eda Josefa \\u201cLeda\\u201d M\\u00e1rquez\\nfather: Francisco Mart\\u00edn Morit\\npartner: Antonio Mar\\u00eda Puello\\nchildren: Antonio Mar\\u00eda Puello Ortiz and Adolfo Morit Puello\\n---\\n\", \"completion\": \"Biography: Sofia Ortiz, known as \\\"The Queen of Vallenato\\\",\\u201cLa Reina de los Vallenato\\\" was born on November 10, 1887, in Valledupar, Cesar. She attended the University of Antioquia, Colombia and she was active from 1915-1947. She was labeled as RCA/Victor and her associated acts were Los Trovadores Zulianos. She won Colombian Songwriters Hall of Fame (2001), Songwriters Hall of Fame (2005), National Order of Merit (Commander) (1987), Grammy Award for Best Tropical Music Album. She was the daughter of Mar\\u00eda Josefa M\\u00e1rquez and Francisco Mart\\u00edn Morit. Sofia grew up in Valledupar, where she lived until she was 16 years old. In 1901, she met and fell in love with Antonio Mar\\u00eda Puello, and married together. They had two children. She died on May 5, 1947 in Medellin, Colombia due to tuberculosis and buried at Cemetery Jardines de Paz.\\n###\"}\n",
            "{\"prompt\": \"notable_type: theologian\\nname: Dana Al-Hussein\\ngender: non-binary\\nnationality: Jordanian\\nbirth_date: 07 July 1926\\nbirth_place: Baghdad, Iraq\\ndeath_date: 10 March, 1965\\ndeath_place: Easton, Connecticut\\ndeath_cause: leukemia; Easton Cemetery\\nalma_mater: Baghdad University\\noccupation: doctor, professor, administrator, theologian\\ntradition_movement: Seventh-Day Adventist\\nnotable_works: Christ Jesus, and the doctrine of the Fathers\\nmain_interests: Sabbath and the Ten Commandments, biblical law applied to 21st century\\nmother: Amina Khayat\\nfather: Jamil Al-Hussein\\npartner: Eleanor P.\\nchildren: none\\n---\\n\", \"completion\": \"Biography: Al-Hussein was born in Baghdad, Iraq on July 7, 1926. Al-Hussein was educated at Baghdad University. He became a professor and later the president of the Middle East Union College of Beirut. Al-Hussein married Eleanor P. and together they had no children. Al-Hussein died on March 10, 1965 in Easton, Connecticut due to leukemia. They buried in Easton Cemetery. Their parents were Amina Khayat and Jamil Al-Hussein.\\n###\"}\n",
            "{\"prompt\": \"notable_type: musician\\nname: Agnete Ingersen\\nbirth_name: Ananaar\\nalias: The Ghost of Greenland\\ngender: non-binary\\nbirth_date: 07 December 1920\\nbirth_place: a little fishing village in the southeast\\ndeath_date: 27 March 2002\\ndeath_place: the village\\ndeath_cause: scurvy / pneumonia\\ninstrument: kalimba\\ngenre: alternative / indie\\nhometown: a small, frozen town on the northwest coast\\nnationality: Greenlandic\\ncitizenship: Greenlandic\\neducation: fishing industry / sea navigation\\nyears_active: 1991-present\\nlabel: The Ghost of Greenland\\nassociated_acts: Ananaar\\nawards: Manifest award (1992, 1994)\\nmother: Martha Ingersen\\nfather: Christian Ingersen\\n---\\n\", \"completion\": \"Biography: Agnete Ingersen (born Ananaar) was born on December 7, 1920, in a small fishing village in the southeast to Martha Ingersen and Christian Ingersen. Ingersen completed education in fishing industry / sea navigation. Their associated acts were Ananaar. Ingersenreceived awards Manifest award (1992, 1994). Ingersen died on 27 March 2002 in the village of scurvy / pneumonia.\\n###\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the fine-tuning API"
      ],
      "metadata": {
        "id": "XZDGRBY5ixfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll make the fine tuning API call via the command line.  Here the -m argument gives the model.  There are 4 sizes of GPT3 models.  They go in alphabetical order from smallest to largest.\n",
        "* Ada \n",
        "* Baddage\n",
        "* Currie\n",
        "* Davinci\n",
        "\n",
        "The models as the model sizes increase, so does their quality and their cost.  Davinci is the highest quality and highest cost model.  I recommend starting by fine-tuning smaller models to debug your code first so that you don't rack up costs.  Once you're sure that your code is working as expected then you can fine-tune a davinci model.\n"
      ],
      "metadata": {
        "id": "YzqdtSXzdXD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t '{fine_tuning_filename}' -m curie\n",
        "#!openai api fine_tunes.create -t '{fine_tuning_filename}' -m davinci"
      ],
      "metadata": {
        "id": "ZJ9-kAe1dWRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b715ba-3830-4301-f553-3bed518571c5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging requires wandb to be installed. Run `pip install wandb`.\n",
            "Upload progress: 100% 132k/132k [00:00<00:00, 198Mit/s]\n",
            "Uploaded file from wikibio_finetuning_data.jsonl: file-pVPsl61CIrLMYuQdYG9fNPJb\n",
            "Created fine-tune: ft-242X8nFrzhXtWDxMAsyIgtD3\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-08-07 21:14:13] Created fine-tune: ft-242X8nFrzhXtWDxMAsyIgtD3\n",
            "[2022-08-07 21:14:17] Fine-tune costs $4.07\n",
            "[2022-08-07 21:14:17] Fine-tune enqueued. Queue number: 0\n",
            "[2022-08-07 21:14:18] Fine-tune started\n",
            "[2022-08-07 21:17:54] Completed epoch 1/4\n",
            "[2022-08-07 21:18:56] Completed epoch 2/4\n",
            "[2022-08-07 21:19:57] Completed epoch 3/4\n",
            "[2022-08-07 21:20:58] Completed epoch 4/4\n",
            "[2022-08-07 21:21:37] Uploaded model: davinci:ft-ccb-lab-members-2022-08-07-21-21-36\n",
            "[2022-08-07 21:21:38] Uploaded result file: file-uXXU96NhfoBkZuONULh7Q3u1\n",
            "[2022-08-07 21:21:38] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m davinci:ft-ccb-lab-members-2022-08-07-21-21-36 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should copy down the fine-tune numbers which look like this:\n",
        "\n",
        "```\n",
        "Created fine-tune: ft-kloUh0jjVc6Jv8p9MfeGHd3s\n",
        "\n",
        "[2022-08-06 00:43:56] Uploaded model: davinci:ft-ccb-lab-members-2022-08-06-00-57-57\n",
        "```\n",
        "\n",
        "If you forget to write it down, you can list your fine-tuned runs and models this way. These model names aren't mneumonic, so it is probably a good idea to make a note on what your model's inputs and outputs are. "
      ],
      "metadata": {
        "id": "CQ8j8VRVdfv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.list"
      ],
      "metadata": {
        "id": "seMeIwOAdgcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run your fine tuned model in the OpenAI Playground.  After the model is finished finetuning you'll find it in the Engine dropdown menu (you might need to press reload in your browser for your fine-tuned model to appear).\n",
        "\n",
        "## Call your fine-tuned model from the OpenAI API\n",
        "\n",
        "Alternately, you can use your fine tuned model via the API by specifying it as the model.  Here's an example:"
      ],
      "metadata": {
        "id": "L_UvcHRUdnWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bio(attributes, finetuned_model):\n",
        "  response = openai.Completion.create(\n",
        "      model=finetuned_model,\n",
        "      prompt=\"{attributes}\\n---\\n\".format(attributes=attributes),\n",
        "      temperature=0.7,\n",
        "      max_tokens=500,\n",
        "      top_p=1,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "      stop=[\"###\"]\n",
        "      )\n",
        "  return response['choices'][0]['text'].strip()\n",
        "\n",
        "# Replace with your model's name\n",
        "finetuned_model = \"davinci:ft-ccb-lab-members-2022-08-07-21-21-36\""
      ],
      "metadata": {
        "id": "sM7AvrzqdjKU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes = \"\"\"\n",
        "notable_type: computer scienist\n",
        "alma_mater: Stanford University (BS in Symbolic Systems), University of Edinburgh (PhD in Informatics)\n",
        "birth_place: California\n",
        "children: 2\n",
        "gender: male\n",
        "main_interests: Artificial Intelligence, Natural Language Processing \n",
        "name: Chris Callison-Burch\n",
        "nationality: American\n",
        "notable_works: Moses: Open source toolkit for statistical machine translation, The Paraphrase Database (PPDB)\n",
        "occupation: professor\n",
        "courses_taught: AI, Crowdsourcing and NLP \n",
        "enrollment_in_most_popular_course: 570 students\n",
        "institution: University of Pennsylvania\n",
        "\"\"\"\n",
        "\n",
        "biography = generate_bio(attributes, finetuned_model)\n",
        "print(attributes)\n",
        "print('---')\n",
        "biography"
      ],
      "metadata": {
        "id": "lMorpMvta66Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze your model's output"
      ],
      "metadata": {
        "id": "ppP6tS3FjBGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes the model will add facts that are not present in the attributes.  For instance, one time it said \n",
        "> He was a member of the research staff at IBM Research in Yorktown Heights.\n",
        "\n",
        "which is not correct. Another time it said\n",
        "> His most popular course was on AI, which had 570 students.\n",
        "\n",
        "which is correct, but not specified in the attirbutes.\n",
        "\n",
        "Try running your own fine-tuned model until it produces something that wasn't licensed by the attributes. \n",
        "\n",
        "Save the good runs and the bad run below."
      ],
      "metadata": {
        "id": "Ith4CGAVdGfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generations_with_correct_facts = [\n",
        "   \"\"\" TODO 1 \"\"\",\n",
        "   \"\"\" TODO 2 \"\"\",\n",
        "                       ]\n",
        "\n",
        "generation_with_incorrect_facts_= \"\"\"\n",
        "\"\"\"\n",
        "\n",
        "incorrect_facts = [\n",
        "    \"\"\" TODO incorrect sentence 1 \"\"\",\n",
        "]"
      ],
      "metadata": {
        "id": "qdFONhDMdWw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune a New Model\n",
        "\n",
        "Now that you've seen an example of how to do fine-tuning with the OpenAI API, let's have you write code to fine-tune your own model.\n",
        "\n",
        "For this model, I'd like you to do the reverse direction of what we just did.  Given a Wikipedia Biograph like this:\n",
        "\n",
        "> Jill Tracy Jacobs Biden (born June 3, 1951) is an American educator and the current first lady of the United States as the wife of President Joe Biden. She was the second lady of the United States from 2009 to 2017. Since 2009, Biden has been a professor of English at Northern Virginia Community College. \n",
        "\n",
        "> She has a bachelor's degree in English and a doctoral degree in education from the University of Delaware, as well as master's degrees in education and English from West Chester University and Villanova University. She taught English and reading in high schools for thirteen years and instructed adolescents with emotional disabilities at a psychiatric hospital. From 1993 to 2008, Biden was an English and writing instructor at Delaware Technical & Community College. Biden is thought to be the first wife of a vice president or president to hold a paying job during her husband's tenure. \n",
        "\n",
        "> Born in Hammonton, New Jersey, she grew up in Willow Grove, Pennsylvania. She married Joe Biden in 1977, becoming stepmother to Beau and Hunter, his two sons from his first marriage. Biden and her husband also have a daughter together, Ashley Biden, born in 1981. She is the founder of the Biden Breast Health Initiative non-profit organization, co-founder of the Book Buddies program, co-founder of the Biden Foundation, is active in Delaware Boots on the Ground, and with Michelle Obama is co-founder of Joining Forces. She has published a memoir and two children's books.\n",
        "\n",
        "Your model should output something like this:\n",
        "```\n",
        "notable_type: First Lady of the United States\n",
        "name: Jill Biden\n",
        "gender: female\n",
        "nationality: American\n",
        "birth_date: 03 June 1951\n",
        "birth_place: Hammonton, New Jersey\n",
        "alma_mater: University of Delaware\n",
        "occupation: professor of English at Northern Virginia Community College\n",
        "notable_works: children's books and memoir\n",
        "main_interests: education, literacy, women's health\n",
        "partner: Joe Biden\n",
        "children: Ashley Biden, Beau Biden (stepson), Hunter Biden (stepson)\n",
        "```\n"
      ],
      "metadata": {
        "id": "yMEUR3f8eh0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def create_wikibio_parser_finetuning_data(wikibios, fine_tuning_filename):\n",
        "  # TODO - write your fine-tuning function\n",
        "  pass\n",
        "\n",
        "fine_tuning_filename='wikibio_parser_finetuning_data.jsonl'\n",
        "create_wikibio_parser_finetuning_data(wiki_bios, fine_tuning_filename)"
      ],
      "metadata": {
        "id": "fnR7bToueV50"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t '{fine_tuning_filename}' -m curie\n",
        "#!openai api fine_tunes.create -t '{fine_tuning_filename}' -m davinci"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzUEaol5eoPG",
        "outputId": "25c37920-cfe6-4f47-eaf1-181ba09de397"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging requires wandb to be installed. Run `pip install wandb`.\n",
            "Upload progress: 100% 132k/132k [00:00<00:00, 182Mit/s]\n",
            "Uploaded file from wikibio_parser_finetuning_data.jsonl: file-5X8WBR8juU7TnGU9QbboFdIg\n",
            "Created fine-tune: ft-kcbvvf5LMMwyoX3gT2GAhX5Z\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-08-07 22:44:10] Created fine-tune: ft-kcbvvf5LMMwyoX3gT2GAhX5Z\n",
            "[2022-08-07 22:44:16] Fine-tune costs $4.07\n",
            "[2022-08-07 22:44:16] Fine-tune enqueued. Queue number: 0\n",
            "[2022-08-07 22:44:18] Fine-tune started\n",
            "[2022-08-07 22:48:01] Completed epoch 1/4\n",
            "[2022-08-07 22:49:04] Completed epoch 2/4\n",
            "[2022-08-07 22:50:08] Completed epoch 3/4\n",
            "[2022-08-07 22:51:09] Completed epoch 4/4\n",
            "[2022-08-07 22:51:46] Uploaded model: davinci:ft-ccb-lab-members-2022-08-07-22-51-46\n",
            "[2022-08-07 22:51:48] Uploaded result file: file-YX4jAegUgLW7n8k0nRDwQ6ee\n",
            "[2022-08-07 22:51:48] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m davinci:ft-ccb-lab-members-2022-08-07-22-51-46 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_bio(biography, finetuned_bio_parser_model):\n",
        "  # TODO call the API with your fine-tuned model, return a string representing the attributes\n",
        "  pass\n",
        "\n",
        "  \n",
        "finetuned_bio_parser_model=\"TODO\""
      ],
      "metadata": {
        "id": "KrqNIFPFmtEQ"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test your parser\n",
        "\n",
        "Next we will test your parser.  This will involve calling your `parse_bio` function about 250 times, so be sure that you've got it properly debugged and working before running this code. "
      ],
      "metadata": {
        "id": "G9V3zLFmqEbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/artificial-intelligence-class/artificial-intelligence-class.github.io/master/homeworks/large-LMs/SynthBio_test.json"
      ],
      "metadata": {
        "id": "v9a9CvE9p8D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_wiki_bio_test_set(filename='SynthBio_test.json', max_test_items=10, randomized=True):\n",
        "  \"\"\" \n",
        "  Loads our wikibio test set, and returns a list of tuples \n",
        "  biographies (text), attributes (dictionaires)\n",
        "  \"\"\"\n",
        "  with open(filename) as f:\n",
        "    synth_bio_data = json.load(f)\n",
        "  bios = []\n",
        "  for data in synth_bio_data:\n",
        "    notable_type = data['notable_type']\n",
        "    attributes = data['attrs']\n",
        "    attributes['notable_type'] = notable_type\n",
        "    biography = data['biographies'][0]\n",
        "    bios.append((biography, attributes))\n",
        "  return bios[:min(max_test_items, len(bios))]\n",
        "\n",
        "\n",
        "def convert_to_dict(predcited_attributes_txt):\n",
        "  \"\"\"\n",
        "  Converts predicted attributes from text format into a dictionary.\n",
        "  \"\"\"\n",
        "  predicted_attributes = {}\n",
        "  for line in predcited_attributes_txt.split('\\n'):\n",
        "    attribute, value = line.split(':')\n",
        "    predicted_attributes[attribute.strip()] = value.strip()\n",
        "  return predicted_attributes\n",
        "\n"
      ],
      "metadata": {
        "id": "Ncu11s25qdoV"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function for computing precision, recall and f-score."
      ],
      "metadata": {
        "id": "giP9b76iFjEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def update_counts(gold_attributes, predicted_attributes, true_positives, false_positives, false_negatives, all_attributes):\n",
        "  # Compute true positives and false negatives\n",
        "  for attribute in gold_attributes:\n",
        "    all_attributes[attribute] += 1\n",
        "    if attribute in predicted_attributes:\n",
        "      # some attributes have multiple values.\n",
        "      gold_values = gold_attributes[attribute].split(',')\n",
        "      for value in gold_values:\n",
        "        if value.strip() in predicted_attributes[attribute]:\n",
        "          true_positives[attribute] += 1\n",
        "        else:\n",
        "          false_negatives[attribute] += 1\n",
        "    else:\n",
        "      false_negatives[attribute] += 1\n",
        "  # Compute false positives \n",
        "  for attribute in predicted_attributes:\n",
        "    if attribute not in gold_attributes:\n",
        "      all_attributes[attribute] += 1\n",
        "    if not attribute in gold_values:\n",
        "      false_positives[attribute] += 1\n",
        "    else:\n",
        "      # some attributes have multiple values.\n",
        "      predicted_values = predicted_attributes[attribute].split(',')\n",
        "      for value in predicted_values:\n",
        "        if value.strip() not in gold_values[attribute]:\n",
        "          false_positives[attribute] += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "8PvGbJYKrEq7"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_on_test_set(finetuned_bio_parser_model, wiki_bio_test, threshold_count = 5):\n",
        "  \"\"\"\n",
        "  Computer the precision, recall and f-score for each of the attributes\n",
        "  that appears more than the treshold count\n",
        "  \"\"\"\n",
        "  true_positives = Counter()\n",
        "  false_positives = Counter()\n",
        "  false_negatives = Counter()\n",
        "  all_attributes = Counter() \n",
        "\n",
        "  for bio, gold_attributes in wiki_bio_test:\n",
        "    predicted_attributes = convert_to_dict(parse_bio(bio, finetuned_bio_parser_model))\n",
        "    update_counts(gold_attributes, predicted_attributes, true_positives, false_positives, false_negatives, all_attributes)  \n",
        "\n",
        "  average_precision = 0\n",
        "  average_recall = 0\n",
        "  total = 0\n",
        "\n",
        "  for attribute in all_attributes:\n",
        "    if all_attributes[attribute] < threshold_count:\n",
        "      continue\n",
        "    print(attribute.upper())\n",
        "    try:\n",
        "      precision = true_positives[attribute] / (true_positives[attribute] + false_positives[attribute])\n",
        "    except: \n",
        "      precision = 0.0\n",
        "    try:\n",
        "      recall = true_positives[attribute] / (true_positives[attribute] + false_negatives[attribute])\n",
        "    except: \n",
        "      recall = 0.0\n",
        "    print(\"precision:\", precision)\n",
        "    print(\"recall:\", recall)\n",
        "    print(\"f-score:\", (precision+recall)/2)\n",
        "    print('---')\n",
        "    average_precision += precision\n",
        "    average_recall += recall\n",
        "    total += 1\n",
        "\n",
        "  print(\"AVERAGE\")\n",
        "  average_precision = average_precision/total\n",
        "  average_recall = average_recall/total\n",
        "  print(\"precision:\", average_precision)\n",
        "  print(\"recall:\", average_recall)\n",
        "  print(\"f-score:\", (average_precision+average_recall)/2)\n",
        "  print('---')\n"
      ],
      "metadata": {
        "id": "M5UXVTgaGHBD"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to evaluate on the full test set, there are 237 test items.  You can set `max_test_items=237`.  Doing so will call your `parse_bio` function about 237 times, so be sure that you've got it properly debugged and working before running this code. "
      ],
      "metadata": {
        "id": "fBCI95rQI8gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_test_items=10\n",
        "wiki_bio_test = load_wiki_bio_test_set(max_test_items)\n",
        "evaluate_on_test_set(finetuned_bio_parser_model, wiki_bio_test, threshold_count = 5)"
      ],
      "metadata": {
        "id": "BDSuk0AWGlOJ"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How well did your model perform?"
      ],
      "metadata": {
        "id": "rCMqucECM9EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - fill in these values\n",
        "average_precision = 0.0\n",
        "average_recall = 0.0\n",
        "average_fscore = 0.0\n",
        "\n",
        "# What attributes had the highest F-scorre\n",
        "best_attributes = {\n",
        "    \"attrbute_name\" : 0.0,\n",
        "}\n",
        "\n",
        "# What attributes had the lowest F-scorre\n",
        "worst_attributes = {\n",
        "    \"attrbute_name\" : 0.0,\n",
        "}\n",
        "\n",
        "# What could you do the perform the model's performance?\n",
        "potential_improvements = \"\"\"\n",
        "TODO\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "O5frXHaeNFjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedback questions"
      ],
      "metadata": {
        "id": "5NFilM6oNv-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many hours did you spend on this assignment? Just an approximation is fine.\n",
        "num_hours_spent = 0\n",
        "\n",
        "# What did you think?  This was the first time we tried this assignment \n",
        "# so you're feedback is valable.\n",
        "feedback = \"\"\"\n",
        "Type your response here.\n",
        "Your response may span multiple lines.\n",
        "Do not include these instructions in your response.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "yPS7_smBN-2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}